{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Big Data Systems and Architectures - Spark Assignment 2021</h2>\n",
    "<h3>Exploring International Flights in 2017 Data</h3>\n",
    "\n",
    "---\n",
    "> Georgia Vlassi p2822001<br />\n",
    "> Business Analytics <br />\n",
    "> Athens University of Economics and Business <br/>\n",
    "\n",
    "---\n",
    "\n",
    "The main scope of this assignment is to analyse a dataset about international flights in 2017, using Apache Spark to reveal insights about these data. \n",
    "\n",
    "You can find the aforementioned data here:\n",
    "http://andrea.imis.athena-innovation.gr/aueb-master/flights.csv.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 1</h3>\n",
    "Your first task is to calculate the average flight delays in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.readwriter import DataFrameReader\n",
    "from pyspark.sql.types import IntegerType, Row, StringType\n",
    "\n",
    "#Import the following to use SQL commands\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import avg,expr, col, desc,round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to create a temporary view to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FlightsAssignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb7ade612e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FlightsAssignment\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initialising the view we have to load our data. The data file named '671009038_T_ONTIME_REPORTING.csv', will be read by using using spark and save them into flights_data variable. In order to access the data download the file mentioned on description and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_data = spark.read\\\n",
    "                    .option(\"header\",\"true\")\\\n",
    "                    .option(\"inferSchema\",\"true\")\\\n",
    "                    .csv(\"671009038_T_ONTIME_REPORTING.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the fisrt 10 records from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|_c19|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|   Asheville, NC| ATL|       Atlanta, GA|    1658|     -7.0|    1758|    -22.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|    New York, NY| RDU|Raleigh/Durham, NC|    1122|     -8.0|    1255|    -29.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|   Cleveland, OH| DTW|       Detroit, MI|    1334|     -7.0|    1417|    -31.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N135EV|     9E|   BHM|  Birmingham, AL| ATL|       Atlanta, GA|    1059|     -1.0|    1255|     -8.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N914XJ|     9E|   GTF| Great Falls, MT| MSP|   Minneapolis, MN|    1057|     -3.0|    1418|    -17.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N924EV|     9E|   GRB|   Green Bay, WI| DTW|       Detroit, MI|     855|      0.0|    1125|     10.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N195PQ|     9E|   AGS|     Augusta, GA| ATL|       Atlanta, GA|     800|     -5.0|     900|    -16.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N319PQ|     9E|   CLT|   Charlotte, NC| JFK|      New York, NY|    1350|    -10.0|    1534|    -29.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N933XJ|     9E|   MEM|     Memphis, TN| MSP|   Minneapolis, MN|    1441|     -4.0|    1641|    -18.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N933XJ|     9E|   MSP| Minneapolis, MN| MEM|       Memphis, TN|     847|     -4.0|    1114|     -6.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- TAIL_NUM: string (nullable = true)\n",
      " |-- CARRIER: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- CARRIER_DELAY: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- SECURITY_DELAY: double (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like SQL we should create a query in spark to calculate the calculate the average flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the average departure delay of flights(the DEP_DELAY column is in minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|AverageDepartureDelay|AverageArrivalDelay|\n",
      "+---------------------+-------------------+\n",
      "|                10.92|               5.41|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AvgDelay = flights_data\\\n",
    "                .agg(round(avg((\"DEP_DELAY\")),2).alias(\"AverageDepartureDelay\"), \n",
    "                     round(avg((\"ARR_DELAY\")),2).alias(\"AverageArrivalDelay\"))\n",
    "AvgDelay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the above with the use of SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wiil convert the above dataframe to a table/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[FL_DATE: string, TAIL_NUM: string, CARRIER: string, ORIGIN: string, ORIGIN_CITY_NAME: string, DEST: string, DEST_CITY_NAME: string, DEP_TIME: int, DEP_DELAY: double, ARR_TIME: int, ARR_DELAY: double, CANCELLED: double, CANCELLATION_CODE: string, DIVERTED: double, CARRIER_DELAY: double, WEATHER_DELAY: double, NAS_DELAY: double, SECURITY_DELAY: double, LATE_AIRCRAFT_DELAY: double, _c19: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_data.createOrReplaceTempView(\"flights_data\")\n",
    "flights_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|AverageDepartureDelay|AverageArrivalDelay|\n",
      "+---------------------+-------------------+\n",
      "|                10.92|               5.41|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AvgDelay_SQL = spark.sql(\"SELECT ROUND(AVG(DEP_DELAY),2) AS AverageDepartureDelay, ROUND(AVG(ARR_DELAY),2) AS AverageArrivalDelay FROM flights_data\")\n",
    "AvgDelay_SQL.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
